# Design space from BERT-tiny to BERT-mini
# Four pre-trained models available at googl-research/bert

datasets:
  - CoLA
  - SST-2
  - MRPC
  - STS-B
  - QQP
  - MNLI-mm
  - QNLI
  - RTE
  - WNLI

architecture:
  hidden_size:
    - 128
    - 256
  attention_heads:
    - 2
    - 4
  encoder_layers:
    - 2
    - 4
  feed-forward_hidden:
    - 1024
    - 2048
    - 4096
  similarity_metric:
    - sdp
    - wma
