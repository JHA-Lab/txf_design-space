# Design space from BERT-tiny to BERT-small
# Nine pre-trained models available at googl-research/bert

datasets:
  - CoLA
  - SST-2
  - MRPC
  - STS-B
  - QQP
  - MNLI-mm
  - QNLI
  - RTE
  - WNLI

architecture:
  hidden_size:
    - 128
    - 256
    - 512
  attention_heads:
    - 10
    - 12
    - 14
    - 16
  encoder_layers:
    - 2
    - 4
    - 6
  feed-forward_hidden:
    - 512
    - 1024
    - 2048
    - 4096
  similarity_metric:
    - sdp
    - wma
    - aa
